#+title: Hands-on_machine_learning_notes
https://homl.info/colab3

* Part I. The Fundamentals of Machine Learning
** 1 The Machine Learning Landscape
*** What Is Machine Learning?
*** Why Use Machine Learning?
*** Examples of Applications
*** Types of Machine Learning Systems
**** Training Supervision
- Supervised learning
- Unsupervised learning
- Semi-supervised learning deal with data that’s partially labeled.
- Self-supervised learning involves actually generating a fully labeled dataset from a fully unlabeled one.
- Reinforcement learning
**** Batch Versus Online Learning
- In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data.
- In online learning, you train the system incrementally by feeding it data instances sequentially, either individually or in small groups called mini-batches. Additionally, online learning algorithms can be used to train models on huge datasets that cannot fit in one machine’s main memory (this is called out-of-core learning).
**** Instance-Based Versus Model-Based Learning
#+begin_src python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
#+end_src

#+begin_src python
from sklearn.neighbors import KNeighborsRegressor

model = KNeighborsRegressor(n_neighbors=3)
#+end_src
*** Main Challenges of Machine Learning
**** Insufficient Quantity of Training Data
**** Nonrepresentative Training Data
**** Poor-Quality Data
*** Irrelevant Features
**** Overfitting the Training Data
- Simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data, or by constraining the model.
- Gather more training data.
- Reduce the noise in the training data (e.g., fix data errors and remove outliers).
Regularization is constraining a model to make it simpler and reduce the risk of
overfitting.
**** Underfitting the Training Data
- Select a more powerful model, with more parameters.
- Feed better features to the learning algorithm (feature engineering).
- Reduce the constraints on the model (for example by reducing the regularization hyperparameter).
**** Stepping Back
- Machine learning is about making machines get better at some task by learning from data, instead of having to explicitly code rules.
- There are many different types of ML systems: supervised or not, batch or online, instance-based or model-based.
- In an ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model-based, it tunes some parameters to fit the model to the training set (i.e., to make good predictions on the training set itself), and then hopefully it will be able to make good predictions on new cases as well. If the algorithm is instance-based, it just learns the examples by heart and generalizes to new instances by using a similarity measure to compare them to the learned instances.
- The system will not perform well if your training set is too small, or if the data is not representative, is noisy, or is polluted with irrelevant features (garbage in, garbage out). Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit).
*** Testing and Validating
You train your model using the training set, and you test it using the test set. The error rate on new cases is called the generalization error (or out-of-sample error), and by evaluating your model on the test set, you get an estimate of this error.
**** Hyperparameter Tuning and Model Selection
Holdout validation: you simply hold out part of the training set to evaluate several candidate models and select the best one. The new held-out set is called the validation set.
**** Data Mismatch
[[./img/1-26.png]]
Figure 1-26. When real data is scarce (right), you may use similar abundant data (left) for training and hold out some of it in a train-dev set to evaluate overfitting; the real data is then used to evaluate data mismatch (dev set) and to evaluate the final model’s performance (test set)
*** Exercises
1. *How would you define machine learning?* Machine Learning is about building systems that can learn from data. Learning means getting better at some task, given some performance measure.
2. *Can you name four types of applications where it shines?* Machine Learning is great for complex problems for which we have no algorithmic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining).
3. *What is a labeled training set?* A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance.
4. *What are the two most common supervised tasks?* The two most common supervised tasks are regression and classification.
5. *Can you name four common unsupervised tasks?* Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning.
6. *What type of algorithm would you use to allow a robot to walk in various unknown terrains?* Reinforcement Learning is likely to perform best if we want a robot to learn to walk in various unknown terrains, since this is typically the type of problem that Reinforcement Learning tackles. It might be possible to express the problem as a supervised or semi-supervised learning problem, but it would be less natural.
7. *What type of algorithm would you use to segment your customers into multiple groups?* If you don't know how to define the groups, then you can use a clustering algorithm (unsupervised learning) to segment your customers into clusters of similar customers. However, if you know what groups you would like to have, then you can feed many examples of each group to a classification algorithm (supervised learning), and it will classify all your customers into these groups.
8. *Would you frame the problem of spam detection as a supervised learning prob‐ lem or an unsupervised learning problem?* Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their labels (spam or not spam).
9. *What is an online learning system?* An online learning system can learn incrementally, as opposed to a batch learning system. This makes it capable of adapting rapidly to both changing data and autonomous systems, and of training on very large quantities of data.
10. *What is out-of-core learning?* Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer's main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these mini-batches.
11. *What type of algorithm relies on a similarity measure to make predictions?* An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a similarity measure to find the most similar learned instances and uses them to make predictions.
12. *What is the difference between a model parameter and a model hyperparameter?* A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply).
13. *What do model-based algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?* Model-based learning algorithms search for an optimal value for the model parameters such that the model will generalize well to new instances. We usually train such systems by minimizing a cost function that measures how bad the system is at making predictions on the training data, plus a penalty for model complexity if the model is regularized. To make predictions, we feed the new instance's features into the model's prediction function, using the parameter values found by the learning algorithm.
14. *Can you name four of the main challenges in machine learning?* Some of the main challenges in Machine Learning are the lack of data, poor data quality, nonrepresentative data, uninformative features, excessively simple models that underfit the training data, and excessively complex models that overfit the data.
15. *If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions?* If a model performs great on the training data but generalizes poorly to new instances, the model is likely overfitting the training data (or we got extremely lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data.
16. *What is a test set, and why would you want to use it?* A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production.
17. *What is the purpose of a validation set?* A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters.
18. *What is the train-dev set, when do you need it, and how do you use it?* The train-dev set is used when there is a risk of mismatch between the training data and the data used in the validation and test datasets (which should always be as close as possible to the data used once the model is in production). The train-dev set is a part of the training set that's held out (the model is not trained on it). The model is trained on the rest of the training set, and evaluated on both the train-dev set and the validation set. If the model performs well on the training set but not on the train-dev set, then the model is likely overfitting the training set. If it performs well on both the training set and the train-dev set, but not on the validation set, then there is probably a significant data mismatch between the training data and the validation + test data, and you should try to improve the training data to make it look more like the validation + test data.
19. *What can go wrong if you tune hyperparameters using the test set?* If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error you measure will be optimistic (you may launch a model that performs worse than you expect).
** 2. End-to-End Machine Learning Project
*Machine Learning Project Checklist (Appendix A)*
1. Frame the problem and look at the big picture.
2. Get the data.
3. Explore the data to gain insights.
4. Prepare the data to better expose the underlying data patterns to machine learning algorithms.
5. Explore many different models and shortlist the best ones.
6. Fine-tune your models and combine them into a great solution.
7. Present your solution.
8. Launch, monitor, and maintain your system.
*** Working with Real Data
*** Look at the Big Picture
**** Frame the Problem
**** Select a Performance Measure
- Root mean square error (RMSE). Computing the root of a sum of squares (RMSE) corresponds to the ℓ2 norm (the Euclidean norm)
- Mean absolute error (MAE). Computing the sum of absolutes (MAE) corresponds to the ℓ1 norm (the Manhattan norm)

RMSE is more sensitive to outliers than the MAE. But when outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs very well and is generally preferred.
**** Check the Assumptions
*** Get the Data
**** Running the Code Examples Using Google Colab
**** Saving Your Code Changes and Your Data
**** The Power and Danger of Interactivity
**** Book Code Versus Notebook Code
**** Download the Data
#+begin_src python
from pathlib import Path
import pandas as pd
import tarfile
import urllib.request
#+end_src
**** Take a Quick Look at the Data Structure
#+begin_src python
import pandas as pd

df = pd.DataFrame(...)
df.head()
df.info()
df["column"].value_counts()
df.describe()
#+end_src

#+begin_src python
import matplotlib.pyplot as plt

df.hist(bins=50, figsize=(12, 8))
#+end_src
**** Create a Test Set
#+begin_src python
import numpy as np
from zlib import crc32  # hash computation
#+end_src

*Train test split*
#+begin_src python
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)
#+end_src

#+begin_src python
from sklearn.model_selection import StratifiedShuffleSplit
# generates n different stratified splits of the same dataset
# split() method yields the training and test indices

df["column_cat"] = pd.cut(df["column"],
    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
    labels=[1, 2, 3, 4, 5])
strat_train_set, strat_test_set = train_test_split(
    df, test_size=0.2, stratify=df["column_cat"], random_state=42)
# you might drop "column_cat" columns here
#+end_src
*** Explore and Visualize the Data to Gain Insights
**** Visualizing Geographical Data
**** Look for Correlations
#+begin_src python
corr_matrix = df.corr()  # standard correlation coefficient (also called Pearson’s r)
corr_matrix["column"].sort_values(ascending=False)
#+end_src

#+begin_src python
from pandas.plotting import scatter_matrix

scatter_matrix(df, figsize=(12, 8))
#+end_src
**** Experiment with Attribute Combinations
*** Prepare the Data for Machine Learning Algorithms
#+begin_src python
df = df.drop("column", axis=1))
#+end_src
**** Clean the Data
Pandas DataFrame’s dropna(), drop(), and fillna() methods:
#+begin_src python
housing.dropna(subset=["column"], inplace=True)  # option 1: Get rid of the corresponding districts.
housing.drop("column", axis=1)  # option 2: Get rid of the whole attribute.
housing["column"].fillna(housing["total_bedrooms"].median(), inplace=True) # option 3: Imputation. Set the missing values to some value (zero, the mean, the median, etc.).
#+end_src

#+begin_src python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")  # (strategy="mean"), (strategy="most_frequent"), ( strategy="constant", fill_value=…)
df_num = df.select_dtypes(include=[np.number])  # select columns with numbers
imputer.fit(df_num)
imputer.statistics_  # median of each attribute
X = imputer.transform(df_num)
#+end_src

You can try KNNImputer and IterativeImputer in sklearn.impute

*Scikit-Learn Design*
1. *Consistency*
   All objects share a consistent and simple interface:
   - Estimators [fit()]
   - Transformers [transform()]
   - Predictors [predict()]
2. *Inspection*
   All the estimator’s hyperparameters are accessible directly via public instance variables (e.g., imputer.strategy), and all the estimator’s learned parameters are accessible via public instance variables with an underscore suffix (e.g., imputer.statistics_).

NumPy output back to DataFrame:
#+begin_src python
X = imputer.transform(column_num)
df_tr = pd.DataFrame(X, columns=column_num.columns,
                     index=column_num.index)
#+end_src
**** Handling Text and Categorical Attributes
*Ordinal Encoder*
#+begin_src python
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
#+end_src

*One Hot Encoder*
#+begin_src python
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)

cat_encoder.categories_ # list of categories

pd.get_dummies(df)  # OneHotEncoder is smarter: it will detect the unknown category and raise an exception.

# If you prefer, you can set the handle_unknown hyperparameter to "ignore"
# it will just represent the unknown category with zeros:
cat_encoder.handle_unknown = "ignore"

# the estimator stores the column names in the feature_names_in_ attribute.
cat_encoder.feature_names_in_ # initial column names
cat_encoder.get_feature_names_out() # new columns
df_output = pd.DataFrame(cat_encoder.transform(df),
                         columns=cat_encoder.get_feature_names_out(),
                         index=df.index)
#+end_src
**** Feature Scaling and Transformation
*Normalization*. Values are shifted and rescaled.
#+begin_src python
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler(feature_range=(-1, 1))
housing_num_min_max_scaled = min_max_scaler.fit_transform(df_num)
#+end_src

*Standardization*, values have a 0 mean and standard deviation is equal to 1.
#+begin_src python
from sklearn.preprocessing import StandardScaler

# with_mean=False to prevent sparse matrix convertion to a dense matrix
# just divides the data by the standard deviation
std_scaler = StandardScaler()
housing_num_std_scaled = std_scaler.fit_transform(housing_num)
#+end_src
*Heavy tails*
1. Make the feature distribution roughly symmetrical
   - square root
   - raising to a power between 0 and 1
   - replacing with logarithm
2. Bucketizing

*Multimodal distribution* (two or more clear peaks)
1. Bucketizing with encoding (e.g. OneHotEncoder)
2. Distances between the value and a mode (RBF)

Radial basis function (*RBF*)—any function that depends only on the distance between the input value and a fixed point.

Gaussian RBF: exp(–γ(x–35)^2)

The hyperparameter γ (gamma) determines how quickly the similarity measure decays as x moves away from 35.
#+begin_src python
from sklearn.metrics.pairwise import rbf_kernel

age_simil_35 = rbf_kernel(housing[["housing_median_age"]], [[35]], gamma=0.1)
#+end_src
[[./img/2-18.png]]
Figure 2-18. Gaussian RBF feature measuring the similarity between the housing median age and 35

*Going back to original scale*
Most of Scikit-Learn’s transformers have an inverse_transform() method.
#+begin_src python
from sklearn.linear_model import LinearRegression

target_scaler = StandardScaler()
scaled_labels = target_scaler.fit_transform()

model = LinearRegression()
model.fit(df, scaled_labels)
some_new_data = df2

scaled_predictions = model.predict(some_new_data)
predictions = target_scaler.inverse_transform(scaled_predictions)
#+end_src
A simpler option is to use a *TransformedTargetRegressor*.
#+begin_src python
from sklearn.compose import TransformedTargetRegressor

model = TransformedTargetRegressor(LinearRegression(),
                                   transformer=StandardScaler())
model.fit(df[["column"]], df_labels)
predictions = model.predict(some_new_data)
#+end_src
**** Custom Transformers
#+begin_src python
from sklearn.preprocessing import FunctionTransformer

# Log example (with inverse_func=)
log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)
log_feature = log_transformer.transform(df[["column"]])

# RBF example (with kw_args=)
rbf_transformer = FunctionTransformer(rbf_kernel,
                                      kw_args=dict(Y=[[35.]], gamma=0.1))
rbf_feature = rbf_transformer.transform(df[["column"]])
#+end_src

FunctionTransformer is not trainable.
To learn parameters in the fit() method and to use them in transform() method you need to write a custom class.
- fit_transform() by adding TransformerMixin as a base class
- get_params() and set_params() for automatic hyperparameter tuning by adding BaseEstimator as a base class (avoid using *args and **kwargs in the constructor)
#+begin_src python
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted # to validate the inputs

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True): # no *args or **kwargs!
        self.with_mean = with_mean

    def fit(self, X, y=None): # y is required even though we don't use it
        X = check_array(X) # checks that X is an array with finite float values
        self.mean_ = X.mean(axis=0)
        self.scale_ = X.std(axis=0)
        self.n_features_in_ = X.shape[1] # every estimator stores this in fit()
        return self # always return self!
    def transform(self, X):
        check_is_fitted(self) # looks for learned attributes (with trailing _)
        X = check_array(X)
        assert self.n_features_in_ == X.shape[1]
        if self.with_mean:
            X = X - self.mean_
        return X / self.scale_
#+end_src
also check:
- feature_names_in_ in the fit()
- get_feature_names_out()
- inverse_transform()

Uses k-means to locate the clusters, then measures the Gaussian RBF similarity between each point and all cluster centers.
#+begin_src python
from sklearn.cluster import KMeans # k-means is a clustering algorithm

class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state

    def fit(self, X, y=None, sample_weight=None):
        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)
        self.kmeans_.fit(X, sample_weight=sample_weight)
        return self # always return self!
    def transform(self, X):
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)
    def get_feature_names_out(self, names=None):
        return [f"Cluster {i} similarity" for i in range(self.n_clusters)]
#+end_src

Check whether your custom estimator respects Scikit-Learn’s API by passing an instance to *check_estimator()*
#+begin_src python
from sklearn.utils.estimator_checks import check_estimator
#+end_src
**** Transformation Pipelines
#+begin_src python
from sklearn.pipeline import Pipeline
num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler()),
])

from sklearn.pipeline import make_pipeline  # If you don’t want to name the transformers
# Using the names of the transformers’ classes, in lowercase and without underscores (e.g., "simpleimputer"):
num_pipeline = make_pipeline(SimpleImputer(strategy="median"), StandardScaler()) # afer fit_transform columns=num_pipeline.get_feature_names_out()
num_prepared = num_pipeline.fit_transform(num)
# if you want to recover a nice DataFrame, you can use the pipeline’s get_feature_names_out() method.
df_num_prepared = pd.DataFrame(
    num_prepared, columns=num_pipeline.get_feature_names_out(),
    index=num.index)
#+end_src

Applying the appropriate transformations to each column:
#+begin_src python
from sklearn.compose import ColumnTransformer

preprocessing = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", cat_pipeline, cat_attribs),
    reminder=pipeline_for_reminded_columns
])
#+end_src

A make_column_selector() function that returns a selector function you can use to automatically select all the features of a given type, such as numerical or categorical:
#+begin_src python
from sklearn.compose import make_column_selector, make_column_transformer
# "pipeline-1" and "pipeline-2" instead of "num" and "cat"

preprocessing = make_column_transformer(
    (num_pipeline, make_column_selector(dtype_include=np.number)),
    (cat_pipeline, make_column_selector(dtype_include=object)),
)

df_prepared = preprocessing.fit_transform(df)
df.get_feature_names_out()
#+end_src
*** Select and Train a Model
**** Train and Evaluate on the Training Set

#+begin_src python
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression

tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))
# forest_reg = make_pipeline(preprocessing, RandomForestRegressor(random_state=42)
# lin_reg = make_pipeline(preprocessing, LinearRegression())
tree_reg.fit(df, df_labels)
df_predictions = tree_reg.predict(df)
#+end_src

Root mean squared error (squared=False)
#+begin_src python
from sklearn.metrics import mean_squared_error

lin_rmse = mean_squared_error(df_labels, df_predictions, squared=False)
#+end_src
**** Better Evaluation Using Cross-Validation
- Splits the training set into k nonoverlapping subsets called folds.
- Trains and evaluates a model k times: 1 fold for evaluation, other folds for training.
- The result is an array containing the k evaluation scores.
#+begin_src python
from sklearn.model_selection import cross_val_score

tree_rmses = -cross_val_score(tree_reg, df, df_labels,
scoring="neg_root_mean_squared_error", cv=10)
#+end_src
*** Fine-Tune Your Model
**** Grid Search
#+begin_src python
from sklearn.model_selection import GridSearchCV

full_pipeline = Pipeline([
    ("preprocessing", preprocessing),
    ("random_forest", RandomForestRegressor(random_state=42)),
])
param_grid = [
    {'preprocessing__geo__n_clusters': [5, 8, 10],
     # preprocessing (ColumnTransformer) -> geo (ClusterSimilarity) -> n_clusters hyperparameter
     'random_forest__max_features': [4, 6, 8]},
    {'preprocessing__geo__n_clusters': [10, 15],
     'random_forest__max_features': [6, 8, 10]},
]
grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,
                           scoring='neg_root_mean_squared_error')
grid_search.fit(housing, housing_labels)
#+end_src
If GridSearchCV is initialized with refit=True (which is the default), then once
it finds the best estimator using cross-validation, it retrains it on the whole
training set.
#+begin_src python
grid_search.best_params_
# {'preprocessing__geo__n_clusters': 15, 'random_forest__max_features': 6}
grid_search.best_estimator_
#+end_src
The evaluation scores. Test scores for each combination of hyperparameters and
for each cross-validation split, as well as the mean test score across all
splits (RMSE in this case).
#+begin_src python
cv_res = pd.DataFrame(grid_search.cv_results_)
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
#+end_src
**** Randomized Search
For each hyperparameter, you must provide either a list of possible values, or a probability distribution
#+begin_src python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50),
                  'random_forest__max_features': randint(low=2, high=20)}

rnd_search = RandomizedSearchCV(
    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,
    scoring='neg_root_mean_squared_error', random_state=42)

rnd_search.fit(housing, housing_labels)
#+end_src
Scikit-Learn also has HalvingRandomSearchCV and HalvingGridSearchCV. The goal is to use the computational resources more efficiently.
**** Ensemble Methods
**** Analyzing the Best Models and Their Errors
RandomForestRegressor can indicate the relative importance of each attribute for making accurate predictions:
#+begin_src python
final_model = rnd_search.best_estimator_ # includes preprocessing
feature_importances = final_model["random_forest"].feature_importances_
#+end_src
Display importance scores next to their corresponding attribute names:
#+begin_src python
sorted(zip(feature_importances,
           final_model["preprocessing"].get_feature_names_out()), reverse=True)
#+end_src
sklearn.feature_selection.SelectFromModel transformer can automatically drop the least useful features for you.
**** Evaluate Your System on the Test Set
95% confidence interval for the generalization error using scipy.stats.t.interval()
#+begin_src python
from scipy import stats

confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
                         loc=squared_errors.mean(),
                         scale=stats.sem(squared_errors)))
#+end_src
*** Launch, Monitor, and Maintain Your System
To save the model, you can use the joblib library:
#+begin_src python
import joblib

joblib.dump(final_model, "my_california_housing_model.pkl")
#+end_src
- Import any custom classes and functions the model relies on
  (which means transferring the code to production).
- Load the model using joblib
- Use it to make predictions
#+begin_src python
import joblib
[...] # import KMeans, BaseEstimator, TransformerMixin, rbf_kernel, etc.

# define custom classes and functions
final_model_reloaded = joblib.load("my_california_housing_model.pkl")

predictions = final_model_reloaded.predict(new_data)
#+end_src
*** Try It Out!
*** Exercises
1. Try a support vector machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel="linear" (with various values for the C hyperparameter) or kernel="rbf" (with various values for the C and gamma hyperparameters). Note that support vector machines don’t scale well to large datasets, so you should probably train your model on just the first 5,000 instances of the training set and use only 3-fold cross-validation, or else it will take hours. Don’t worry about what the hyperparameters mean for now; we’ll discuss them in Chapter 5. How does the best SVR predictor perform?
2. Try replacing the GridSearchCV with a RandomizedSearchCV.
3. Try adding a SelectFromModel transformer in the preparation pipeline to select only the most important attributes.
4. Try creating a custom transformer that trains a k-nearest neighbors regressor (sklearn.neighbors.KNeighborsRegressor) in its fit() method, and outputs the model’s predictions in its transform() method. Then add this feature to the preprocessing pipeline, using latitude and longitude as the inputs to this transformer. This will add a feature in the model that corresponds to the housing median price of the nearest districts.
5. Automatically explore some preparation options using GridSearchCV.
6. Try to implement the StandardScalerClone class again from scratch, then add support for the inverse_transform() method: executing scaler. inverse_transform(scaler.fit_transform(X)) should return an array very close to X. Then add support for feature names: set feature_names_in_ in the fit() method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the get_feature_names_out() method: it should have one optional input_features=None argument. If passed, the method should check that its length matches n_features_in_, and it should match feature_names_in_ if it is defined; then input_features should be returned. If input_features is None, then the method should either return feature_names_in_ if it is defined or np.array(["x0", "x1", ...]) with length n_features_in_ otherwise.
** 3. Classification
*** MNIST
*** Training a Binary Classifier
#+begin_src python
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train_5)
#+end_src
*** Performance Measures
**** Measuring Accuracy Using Cross-Validation
#+begin_src python
from sklearn.model_selection import cross_val_score

cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")

from sklearn.dummy import DummyClassifier
#+end_src
Accuracy is generally not the preferred performance measure for classifiers. A much better way to evaluate the performance of a classifier is to look at the confusion matrix.
**** Confusion Matrices
#+begin_src python
from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)
#+end_src
The model makes predictions on data that it never saw during training.

#+begin_src python
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_train_5, y_train_pred)
#+end_src
*Row:* an actual class, *column:* a predicted class.
|                    | *predicted as negative*             | *predicted as positive*            |
|--------------------+-------------------------------------+------------------------------------|
| *actualy negative* | true negative (TN)                  | false positive (FP), type I errors |
| *actualy positive* | false negative (FN), type II errors | true positive (TP)                 |
- \(\text{precision} = \frac{TP}{(TP + FP)}\)
- \(\text{recall (sensitivity, the true positive rate (TPR))} = \frac{TP}{(TP + FN)}\)
[[./img/3-3.png]]
Figure 3-3. An illustrated confusion matrix showing examples of true negatives
(top left), false positives (top right), false negatives (lower left), and true
positives (lower right)
**** Precision and Recall
#+begin_src python
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import f1_score

precision_score(y_train_5, y_train_pred) # == 3530 / (687 + 3530)
recall_score(y_train_5, y_train_pred) # == 3530 / (1891 + 3530)
f1_score(y_train_5, y_train_pred)
#+end_src
The F1 score is the harmonic mean of precision and recall. The harmonic mean gives much more weight to low values.

F1 = 2/((1/precision)+(1/recall)) = 2*(precision*recall)/(precision+recall) = TP/(TP+(FN+FP)/2)

\(F_1 =
\frac{2}{\frac{1}{\text{precision}}+\frac{1}{\text{recall}}} =
2 \times \frac{\text{precision}\times\text{recall}}{\text{precision} + \text{recall}} =
\frac{TP}{TP+\frac{FN+FP}{2}}\)
**** The Precision/Recall Trade-off
**** The ROC Curve
*** Multiclass Classification
*** Error Analysis
*** Multilabel Classification
*** Multioutput Classification
*** Exercises
** 4. Training Models
*** Linear Regression
**** The Normal Equation
**** Computational Complexity
*** Gradient Descent
**** Batch Gradient Descent
**** Stochastic Gradient Descent
**** Mini-Batch Gradient Descent
*** Polynomial Regression
*** Learning Curves
*** Regularized Linear Models
**** Ridge Regression
**** Lasso Regression
**** Elastic Net Regression
**** Early Stopping
*** Logistic Regression
**** Estimating Probabilities
**** Training and Cost Function
**** Decision Boundaries
**** Softmax Regression
*** Exercises
** 5. Support Vector Machines
*** Linear SVM Classification
**** Soft Margin Classification
*** Nonlinear SVM Classification
**** Polynomial Kernel
**** Similarity Features
**** Gaussian RBF Kernel
**** SVM Classes and Computational Complexity
*** SVM Regression
*** Under the Hood of Linear SVM Classifiers
*** The Dual Problem
**** Kernelized SVMs
*** Exercises
** 6. Decision Trees
*** Training and Visualizing a Decision Tree
*** Making Predictions
*** Estimating Class Probabilities
*** The CART Training Algorithm
*** Computational Complexity
*** Gini Impurity or Entropy?
*** Regularization Hyperparameters
*** Regression
*** Sensitivity to Axis Orientation
*** Decision Trees Have a High Variance
*** Exercises
** 7. Ensemble Learning and Random Forests
*** Voting Classifiers
*** Bagging and Pasting
**** Bagging and Pasting in Scikit-Learn
**** Out-of-Bag Evaluation
**** Random Patches and Random Subspaces
*** Random Forests
**** Extra-Trees
**** Feature Importance
*** Boosting
**** AdaBoost
**** Gradient Boosting
**** Histogram-Based Gradient Boosting
*** Stacking
*** Exercises
** 8. Dimensionality Reduction
*** The Curse of Dimensionality
*** Main Approaches for Dimensionality Reduction
**** Projection
**** Manifold Learning
*** PCA
**** Preserving the Variance
**** Principal Components
**** Projecting Down to d Dimensions
**** Using Scikit-Learn
**** Explained Variance Ratio
**** Choosing the Right Number of Dimensions
**** PCA for Compression
**** Randomized PCA
**** Incremental PCA
*** Random Projection
*** LLE
*** Other Dimensionality Reduction Techniques
*** Exercises
** 9. Unsupervised Learning Techniques
*** Clustering Algorithms: k-means and DBSCAN
**** k-means
**** Limits of k-means
**** Using Clustering for Image Segmentation
**** Using Clustering for Semi-Supervised Learning
**** DBSCAN
**** Other Clustering Algorithms
*** Gaussian Mixtures
**** Using Gaussian Mixtures for Anomaly Detection
**** Selecting the Number of Clusters
**** Bayesian Gaussian Mixture Models
**** Other Algorithms for Anomaly and Novelty Detection
*** Exercises
* Part II. Neural Networks and Deep Learning
** 10. Introduction to Artificial Neural Networks with Keras
*** From Biological to Artificial Neurons
**** Biological Neurons
**** Logical Computations with Neurons
**** The Perceptron
**** The Multilayer Perceptron and Backpropagation
**** Regression MLPs
**** Classification MLPs
*** Implementing MLPs with Keras
**** Building an Image Classifier Using the Sequential API
**** Building a Regression MLP Using the Sequential API
**** Building Complex Models Using the Functional API
**** Using the Subclassing API to Build Dynamic Models
**** Saving and Restoring a Model
**** Using Callbacks
**** Using TensorBoard for Visualization
*** Fine-Tuning Neural Network Hyperparameters
**** Number of Hidden Layers
**** Number of Neurons per Hidden Layer
**** Learning Rate, Batch Size, and Other Hyperparameters
*** Exercises
** 11. Training Deep Neural Networks
*** The Vanishing/Exploding Gradients Problems
**** Glorot and He Initialization
**** Better Activation Functions
**** Batch Normalization
**** Gradient Clipping
*** Reusing Pretrained Layers
**** Transfer Learning with Keras
**** Unsupervised Pretraining
**** Pretraining on an Auxiliary Task
*** Faster Optimizers
**** Momentum
**** Nesterov Accelerated Gradient
**** AdaGrad
**** RMSProp
**** Adam
**** AdaMax
**** Nadam
**** AdamW
*** Learning Rate Scheduling
*** Avoiding Overfitting Through Regularization
**** ℓ1 and ℓ2 Regularization
**** Dropout
**** Monte Carlo (MC) Dropout
**** Max-Norm Regularization
*** Summary and Practical Guidelines
*** Exercises
** 12. Custom Models and Training with TensorFlow
*** A Quick Tour of TensorFlow
*** Using TensorFlow like NumPy
**** Tensors and Operations
**** Tensors and NumPy
**** Type Conversions
**** Variables
**** Other Data Structures
*** Customizing Models and Training Algorithms
**** Custom Loss Functions
**** Saving and Loading Models That Contain Custom Components
**** Custom Activation Functions, Initializers, Regularizers, and Constraints
**** Custom Metrics
**** Custom Layers
**** Custom Models
**** Losses and Metrics Based on Model Internals
**** Computing Gradients Using Autodiff
**** Custom Training Loops
*** TensorFlow Functions and Graphs
**** AutoGraph and Tracing
**** TF Function Rules
*** Exercises
** 13. Loading and Preprocessing Data with TensorFlow
*** The tf.data API
**** Chaining Transformations
**** Shuffling the Data
**** Interleaving Lines from Multiple Files
**** Preprocessing the Data
**** Putting Everything Together
**** Prefetching
**** Using the Dataset with Keras
*** The TFRecord Format
**** Compressed TFRecord Files
**** A Brief Introduction to Protocol Buffers
**** TensorFlow Protobufs
**** Loading and Parsing Examples
**** Handling Lists of Lists Using the SequenceExample Protobuf
*** Keras Preprocessing Layers
**** The Normalization Layer
**** The Discretization Layer
**** The CategoryEncoding Layer
**** The StringLookup Layer
**** The Hashing Layer
**** Encoding Categorical Features Using Embeddings
**** Text Preprocessing
**** Using Pretrained Language Model Components
**** Image Preprocessing Layers
*** The TensorFlow Datasets Project
*** Exercises
** 14. Deep Computer Vision Using Convolutional Neural Networks
*** The Architecture of the Visual Cortex
*** Convolutional Layers
**** Filters
**** Stacking Multiple Feature Maps
**** Implementing Convolutional Layers with Keras
**** Memory Requirements
*** Pooling Layers
*** Implementing Pooling Layers with Keras
*** CNN Architectures
**** LeNet-5
**** AlexNet
**** GoogLeNet
**** VGGNet
**** ResNet
**** Xception
**** SENet
**** Other Noteworthy Architectures
**** Choosing the Right CNN Architecture
*** Implementing a ResNet-34 CNN Using Keras
*** Using Pretrained Models from Keras
*** Pretrained Models for Transfer Learning
*** Classification and Localization
*** Object Detection
**** Fully Convolutional Networks
**** You Only Look Once
*** Object Tracking
*** Semantic Segmentation
*** Exercises
** 15. Processing Sequences Using RNNs and CNNs
*** Recurrent Neurons and Layers
**** Memory Cells
**** Input and Output Sequences
*** Training RNNs
*** Forecasting a Time Series
**** The ARMA Model Family
**** Preparing the Data for Machine Learning Models
**** Forecasting Using a Linear Model
**** Forecasting Using a Simple RNN
**** Forecasting Using a Deep RNN
**** Forecasting Multivariate Time Series
**** Forecasting Several Time Steps Ahead
**** Forecasting Using a Sequence-to-Sequence Model
*** Handling Long Sequences
**** Fighting the Unstable Gradients Problem
**** Tackling the Short-Term Memory Problem
*** Exercises
** 16. Natural Language Processing with RNNs and Attention
*** Generating Shakespearean Text Using a Character RNN
**** Creating the Training Dataset
**** Building and Training the Char-RNN Model
**** Generating Fake Shakespearean Text
**** Stateful RNN
*** Sentiment Analysis
**** Masking
**** Reusing Pretrained Embeddings and Language Models
*** An Encoder–Decoder Network for Neural Machine Translation
**** Bidirectional RNNs
**** Beam Search
*** Attention Mechanisms
**** Attention Is All You Need: The Original Transformer Architecture
*** An Avalanche of Transformer Models
*** Vision Transformers
*** Hugging Face’s Transformers Library
*** Exercises
** 17. Autoencoders, GANs, and Diffusion Models
*** Efficient Data Representations
*** Performing PCA with an Undercomplete Linear Autoencoder
*** Stacked Autoencoders
**** Implementing a Stacked Autoencoder Using Keras
**** Visualizing the Reconstructions
**** Visualizing the Fashion MNIST Dataset
**** Unsupervised Pretraining Using Stacked Autoencoders
**** Tying Weights
**** Training One Autoencoder at a Time
*** Convolutional Autoencoders
*** Denoising Autoencoders
*** Sparse Autoencoders
*** Variational Autoencoders
*** Generating Fashion MNIST Images
*** Generative Adversarial Networks
**** The Difficulties of Training GANs
**** Deep Convolutional GANs
**** Progressive Growing of GANs
**** StyleGANs
*** Diffusion Models
*** Exercises
** 18. Reinforcement Learning
*** Learning to Optimize Rewards
*** Policy Search
*** Introduction to OpenAI Gym
*** Neural Network Policies
*** Evaluating Actions: The Credit Assignment Problem
*** Policy Gradients
*** Markov Decision Processes
*** Temporal Difference Learning
*** Q-Learning
**** Exploration Policies
**** Approximate Q-Learning and Deep Q-Learning
*** Implementing Deep Q-Learning
*** Deep Q-Learning Variants
**** Fixed Q-value Targets
**** Double DQN
**** Prioritized Experience Replay
**** Dueling DQN
*** Overview of Some Popular RL Algorithms
*** Exercises
** 19. Training and Deploying TensorFlow Models at Scale
*** Serving a TensorFlow Model
**** Using TensorFlow Serving
**** Creating a Prediction Service on Vertex AI
**** Running Batch Prediction Jobs on Vertex AI
*** Deploying a Model to a Mobile or Embedded Device
*** Running a Model in a Web Page
*** Using GPUs to Speed Up Computations
**** Getting Your Own GPU
**** Managing the GPU RAM
**** Placing Operations and Variables on Devices
**** Parallel Execution Across Multiple Devices
*** Training Models Across Multiple Devices
**** Model Parallelism
**** Data Parallelism
**** Training at Scale Using the Distribution Strategies API
**** Training a Model on a TensorFlow Cluster
**** Running Large Training Jobs on Vertex AI
**** Hyperparameter Tuning on Vertex AI
*** Exercises
*** Thank You!
** A. Machine Learning Project Checklist
** B. Autodiff
** C. Special Data Structures
** D. TensorFlow Graphs
** Index
